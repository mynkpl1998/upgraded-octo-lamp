# To set value to None for a parameter. Leave it blank

EXP_NAME:
  # ---- Checkpoint Frequency ---- #
  checkpoint_freq: 100
  # ---- Checkpoint Frequency ---- #
  config:

    # ---- Generalized Advantage Estimator Parameters ---- #
    # If true, use the Generalized Advantage Estimator (GAE)
    use_gae: true
    # GAE(lambda) parameter
    lambda: 0.95
    # ---- Generalized Advantage Estimator Parameters ---- #


    # ---- PPO Configs ---- #
    # Initial coefficient for KL divergence
    kl_coeff: 0.1
    # PPO clip parameter
    clip_param: 0.2
    # Target value for KL divergence
    kl_target: 0.01
    # Whether to rollout "complete_episodes" or "truncate_episodes"
    batch_mode: "truncate_episodes"
    # Forget Factor
    gamma: 0.99
    # ---- PPO Configs ---- #


    # ---- SGD Configs ---- #
    # Size of batches collected from each worker
    sample_batch_size : 100
    # Total SGD batch size across all devices for SGD
    sgd_minibatch_size: 4086
    # Number of timesteps collected for each SGD round (leave this, script will autofill this according to best value)
    train_batch_size:
    # Whether to shuffle sequences in the batch when training (recommended)
    # shuffle_sequences: true (not implemented in release 0.7.2)
    # Number of SGD iterations in each outer loop
    num_sgd_iter: 30
    # ---- SGD Configs ---- #


    # ---- Optimizer Configs ---- #
    # Stepsize of SGD
    lr: 0.00005
    # Learning rate schedule
    lr_schedule:
    # Uses the sync samples optimizer instead of the multi-gpu one. This does
    # not support minibatches.
    simple_optimizer: false
    # ---- Optimizer Configs ---- #


    # ---- Value Function Configs ---- #
    # Share layers for value function. If you set this to True, it's important
    # to tune vf_loss_coeff.
    vf_share_layers: true
    #Coefficient of the value function loss. It's important to tune this if
    #you set vf_share_layers: False
    vf_loss_coeff: 0.001
    # Clip param for the value function. Note that this is sensitive to the
    # scale of the rewards. If your expected V is large, increase this.
    vf_clip_param: 1000.0
    # ---- Value Function Configs ---- #

    
    # ---- Policy Entropy Regularizer ---- #
    # Coefficient of the entropy regularizer
    entropy_coeff: 0.01
    # Decay schedule for the entropy regularizer
    #entropy_coeff_schedule: None (not implemented in release 0.7.2)
    # ---- Policy Entropy Regularizer ---- #


    # ---- Misc ---- #
    # Gradient Clipping
    grad_clip: 
    # Which observation filter to apply to the observation
    observation_filter: "NoFilter"
    # Max length of episode
    horizon: 3350
    # ---- Misc ---- #


    # ---- Compute Resources ---- #
    # Set number of workers to spawn
    num_workers: 10
    # Set number of gpus to use
    num_gpus : 1
    # ---- Compute Resources ---- #
    
    
    # ---- Neutral Net Configs ---- #
    model:
      max_seq_len: 16
      lstm_cell_size: 128
      fcnet_hiddens: [128, 128]
      fcnet_activation: "relu"
      lstm_use_prev_action_reward: false
      framestack: false
      grayscale: false
      zero_mean: false
    # ---- Neutral Net Configs ---- #
